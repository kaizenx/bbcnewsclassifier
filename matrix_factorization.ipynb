{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "100e46bf-3e96-49f0-816a-9523c3816745",
   "metadata": {},
   "source": [
    "# Week 4 Matrix Factorization\n",
    "Movie Ratings Data\n",
    "Please download movie data and put it into data_movies folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16e2a956-6585-4b3a-88bb-90307bd51a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb77e5-a189-4bc5-9cdf-1b157ed08f82",
   "metadata": {},
   "source": [
    "## Step 1: Load Data\n",
    "Make sure your CSV files are loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa6ccd4-b417-4bae-91ce-bfff673019e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data2/users.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m MV_users \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata2/users.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m MV_movies \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata2/movies.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata2/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data2/users.csv'"
     ]
    }
   ],
   "source": [
    "MV_users = pd.read_csv('data_movies/users.csv')\n",
    "MV_movies = pd.read_csv('data_movies/movies.csv')\n",
    "train = pd.read_csv('data_movies/train.csv')\n",
    "test = pd.read_csv('data_movies/test.csv')\n",
    "\n",
    "print(test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294a2b22-29cf-4a89-8f16-f60e5891e368",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the Data\n",
    "We'll create a user-item matrix for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ce9c2-11f7-40e2-a804-a63984a4a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user-item matrix\n",
    "train_matrix = train.pivot(index='uID', columns='mID', values='rating')\n",
    "\n",
    "# Fill missing values with 0 (for matrix factorization purposes)\n",
    "train_matrix_filled = train_matrix.fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983439d4-86a6-4662-bae4-518dbf392c00",
   "metadata": {},
   "source": [
    "## Step 3: Apply Matrix Factorization\n",
    "We'll use Non-negative Matrix Factorization (NMF) as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c1a4c6-8c94-474e-9b07-6c7270cd40e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Apply NMF\n",
    "nmf_model = NMF(n_components=20, init='random', random_state=42, max_iter=500)\n",
    "W = nmf_model.fit_transform(train_matrix_filled)\n",
    "H = nmf_model.components_\n",
    "\n",
    "# Reconstruct the ratings matrix\n",
    "predicted_ratings = np.dot(W, H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a62f5b3-38c8-4aef-a1d4-2eea610c6b02",
   "metadata": {},
   "source": [
    "## Step 4: Predict Missing Ratings\n",
    "Now, predict the ratings for the test set using the reconstructed matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e885ec3-f15a-46a7-b040-5366d6f7258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the predicted ratings matrix to DataFrame for easy access\n",
    "predicted_ratings_df = pd.DataFrame(predicted_ratings, index=train_matrix.index, columns=train_matrix.columns)\n",
    "\n",
    "# Prepare the test data\n",
    "# test['predicted_rating'] = test.apply(lambda row: predicted_ratings_df.loc[row['uID'], row['mID']], axis=1)\n",
    "\n",
    "print(predicted_ratings_df.index)  # Check the user IDs in the predicted ratings\n",
    "print(predicted_ratings_df.columns)  # Check the movie IDs in the predicted ratings\n",
    "print(test['uID'].unique())  # Check the unique user IDs in the test set\n",
    "print(test['mID'].unique())  # Check the unique movie IDs in the test set\n",
    "\n",
    "test['uID'] = test['uID'].astype(predicted_ratings_df.index.dtype)\n",
    "test['mID'] = test['mID'].astype(predicted_ratings_df.columns.dtype)\n",
    "\n",
    "def get_predicted_rating(row):\n",
    "    try:\n",
    "        return predicted_ratings_df.loc[row['uID'], row['mID']]\n",
    "    except KeyError:\n",
    "        return np.nan  # Or some default value\n",
    "\n",
    "test['predicted_rating'] = test.apply(get_predicted_rating, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6cd276-dfc2-448f-8168-faa757c954a2",
   "metadata": {},
   "source": [
    "## Step 5: Measure RMSE\n",
    "Finally, calculate the RMSE between the actual and predicted ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3772de-057e-43d4-a16c-4eeee8d7a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test = test.dropna(subset=['predicted_rating'])\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(test['rating'], test['predicted_rating']))\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6358be2-7d7b-4971-b681-fcc7e59bd3f9",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "### Discussion of Results\n",
    "\n",
    "The RMSE of 2.8538 suggests that the Non-Negative Matrix Factorization (NMF) model didn't perform as well as expected. Here are some reasons why this might be the case:\n",
    "\n",
    "### 1. **Complexity of NMF:**\n",
    "   - **NMF Limitations:** NMF is a linear model that assumes non-negativity in both the factors and the original matrix. While it can be effective for certain types of data, its linear nature might not capture the more complex interactions between users and items (movies) as well as other methods.\n",
    "   - **Overfitting:** NMF might also overfit the training data, especially if the number of components is not appropriately tuned. This overfitting can lead to poorer performance on the test set.\n",
    "   - **Sensitivity to Sparsity:** Movie ratings datasets are typically sparse, with many missing ratings. NMF may struggle to accurately reconstruct the user-item matrix if the data is too sparse, leading to higher errors.\n",
    "\n",
    "### 2. **Comparison to Baseline or Similarity-Based Methods:**\n",
    "   - **Baseline Models:** Simple baseline methods, such as predicting the mean rating for a movie or the average rating a user gives, can sometimes outperform more complex models when the data is sparse or the underlying patterns are simple.\n",
    "   - **Similarity-Based Methods:** Methods like user-user or item-item collaborative filtering directly leverage similarities between users or items. These methods can be more effective when the relationships between users and items are straightforward or when there are clear clusters of similar users/items.\n",
    "\n",
    "### Ways to Improve the NMF Model\n",
    "\n",
    "Here are some suggestions to improve the performance of NMF or alternative methods you might consider:\n",
    "\n",
    "### 1. **Hyperparameter Tuning:**\n",
    "   - **Number of Components:** Experiment with different numbers of latent components in the NMF model. Too few components might underfit, while too many might overfit.\n",
    "   - **Regularization:** Introduce regularization to prevent overfitting. NMF in `sklearn` allows for regularization on both the components and the coefficient matrix, which can help control the complexity of the model.\n",
    "\n",
    "### 2. **Hybrid Methods:**\n",
    "   - **Combine NMF with Similarity-Based Methods:** Consider combining NMF with user-based or item-based collaborative filtering. For example, you can use NMF to get initial estimates of ratings and then refine those estimates using similarity-based adjustments.\n",
    "   - **Blending Models:** Blend predictions from NMF with those from simpler models like baseline predictors or similarity-based methods. This ensemble approach can leverage the strengths of multiple methods.\n",
    "\n",
    "### 3. **Data Preprocessing:**\n",
    "   - **Imputation:** Before applying NMF, experiment with different imputation strategies for missing data. For example, filling in missing values with a global mean, item-specific mean, or user-specific mean might help NMF perform better.\n",
    "   - **Feature Engineering:** Introduce additional features, such as user demographics or movie genres, that can be incorporated into the model, either directly or as part of a hybrid approach.\n",
    "\n",
    "### 4. **Try Alternative Matrix Factorization Techniques:**\n",
    "   - **SVD (Singular Value Decomposition):** SVD is another matrix factorization technique that might perform better on certain datasets, especially if you relax the non-negativity constraint.\n",
    "   - **ALS (Alternating Least Squares):** ALS is commonly used in collaborative filtering, especially in large-scale recommendation systems, and might be more robust to sparsity.\n",
    "\n",
    "### 5. **Model Evaluation and Cross-Validation:**\n",
    "   - **Cross-Validation:** Ensure that you are using proper cross-validation techniques to evaluate the performance of your model. This will help in obtaining a more reliable estimate of the model's true performance.\n",
    "   - **Learning Curve Analysis:** Perform a learning curve analysis to understand how the model's performance scales with more data. This can help identify whether the model is underfitting or overfitting.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "While NMF can be a powerful tool, it may not always be the best choice for all datasets, particularly when the data is sparse or the relationships between users and items are complex. By experimenting with hyperparameters, combining models, and trying alternative techniques, you can improve performance and potentially achieve lower RMSE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e520de7-e183-450b-bd17-d50c7f5aa4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
